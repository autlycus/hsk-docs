Apache Hadoop Projects
----------------------

Apache Hadoop is an open source, batch data processing system for
enormous amounts of data.  Hadoop runs as a platform that provides
cost-effective, scalable infrastructure for building Big Data analytic
applications. All Hadoop clusters contain a distributed file system
called the Hadoop Distributed File System (HDFS), a computation layer
called MapReduce.

The Apache Hadoop project contains the following subprojects:


- **Hadoop Distributed File System (HDFS)** – A distributed
  file system that provides high-throughput access to application data.

- **Hadoop MapReduce** – A software framework for writing
  applications to reliably process large amounts of data in parallel
  across a cluster.

Hadoop is supplemented by an ecosystem of Apache projects, such as Pig,
Hive, Sqoop, Flume, Oozie, Whirr, HBase, and Zookeeper that extend the
value of Hadoop and improves its usability.

Version 2 of Apache Hadoop introduces YARN, a sub-project of Hadoop that
separates the resource management and processing components. YARN was
born of a need to enable a broader array of interaction patterns for
data stored in HDFS beyond MapReduce. The YARN-based architecture of
Hadoop 2.0 provides a more general processing platform that is not
constrained to MapReduce.

For full details of the `Apache Hadoop project <http://hadoop.apache.org>`_
